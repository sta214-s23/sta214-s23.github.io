---
title: Logistic regression assumptions and diagnostics
output:
  xaringan::moon_reader:
    css: "lab-slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

### Recap: quantile residual plots

.large[
```{r, include=F}
library(tidyverse)
library(statmod)

# simulate a single explanatory variable from a Normal distribution
x <- rnorm(1000)

# create P(Y = 1 | X) for each entry in x
# Here log odds = -1 + 2x
p <- exp(-1 + 2*x)/(1 + exp(-1 + 2*x))

# Finally, simulate a binary response at each x
y <- rbinom(1000, 1, p)

# fit the model and plot the quantile residuals against x
# add a smooth fit to see if there is a relationship
m1 <- glm(y ~ x, family = binomial)
p1 <- data.frame(x = x, residuals = qresid(m1)) %>%
  ggplot(aes(x = x, y = residuals)) +
  geom_point() +
  geom_smooth() +
  theme_bw() +
  labs(title = "Shape assumption satisfied")


p <- exp(-1 + 2*x^2)/(1 + exp(-1 + 2*x^2))

# Finally, simulate a binary response at each x
y <- rbinom(1000, 1, p)

# fit the model and plot the quantile residuals against x
# add a smooth fit to see if there is a relationship
m1 <- glm(y ~ x, family = binomial)
p2 <- data.frame(x = x, residuals = qresid(m1)) %>%
  ggplot(aes(x = x, y = residuals)) +
  geom_point() +
  geom_smooth() +
  theme_bw() +
  labs(title = "Shape assumption violated")
```

```{r, message=F, echo=F, fig.align='center', fig.width=10, fig.height=4}
library(gridExtra)
grid.arrange(p1, p2, ncol=2)
```


]

---


### Multicollinearity

.large[
.question[
What is multicollinearity?
]
]

---

### Class activity, Part I

.large[
[https://sta214-s23.github.io/class_activities/ca_lecture_11.html](https://sta214-s23.github.io/class_activities/ca_lecture_11.html)

.question[
* Simulate correlated data
* Assess the impact on estimated coefficients
]
]

---

### The impact of multicollinearity

.large[
.question[
How does correlation between the explanatory variables impact the fitted model?
]
]

---

### Example: College scorecard data

.large[
The Department of Education compiles the College Scorecard, which is used to help prospective college students compare schools. For each school, variables include:

* CONTROL: whether the school is public or private
* SATVRMID: midpoint of SAT critical reading scores of students attending the school
* ACTCMMID: midpoint of the ACT cumulative scores
* UGDS: number of undergraduate students at the school
* NPT4: average cost to attend the school
* PCTFLOAN: fraction of undergraduates receiving a federal student loan
* MD_EARN_WNE_P10: median salary of students 10 years after graduation
]

---

### Example: College scorecard data

.large[

* CONTROL: whether the school is public or private
* SATVRMID: midpoint of SAT critical reading scores of students attending the school
* ACTCMMID: midpoint of the ACT cumulative scores
* UGDS: number of undergraduate students at the school
* NPT4: average cost to attend the school
* PCTFLOAN: fraction of undergraduates receiving a federal student loan
* MD_EARN_WNE_P10: median salary of students 10 years after graduation

.question[
Which of these variables may suffer from multicollinearity?
]
]

---

### Diagnosing multicollinearity

.large[
.question[
How do I detect multicollinearity in my data?
]
]

---

### Scatterplot and correlation matrix

```{r, include=F}
library(tidyverse)
library(GGally)
scorecard <- read_csv("https://sta112-s22.github.io/projects/scorecard.csv")
```

```{r, message=F, warning=F, echo=F, fig.align='center', fig.width=10, fig.height=7}
scorecard %>%
  ggpairs(columns = 4:10,
          upper = list(continuous = "points"),
          lower = list(continuous = GGally::wrap(ggally_cor, stars = F))) +
  theme_classic()
```

---

### Variance inflation factors

---

### Variance inflation factors in R

```{r, message=F, warning=F}
library(car)

m1 <- glm(RPY_3YR_70 ~ CONTROL + SATVRMID + 
            SATMTMID + SATWRMID + ACTCMMID + ACTENMID + 
            ACTMTMID + ACTWRMID + UGDS +
            PCTFLOAN + MD_EARN_WNE_P10 + NPT4,
          data = scorecard, family = binomial)
vif(m1)
```


---

### Addressing model issues

.large[
.question[
How should we handle multicollinearity in the explanatory variables? Discuss with a neighbor for a few minutes, then we will discuss as a group.
]
]

---

### Outliers and influential points

.large[
.question[
What is an outlier?
]
]

---

### Class activity, Part II

.large[
[https://sta214-s23.github.io/class_activities/ca_lecture_11.html](https://sta214-s23.github.io/class_activities/ca_lecture_11.html)

.question[
* Simulate data with a potential outlier
* Assess the impact on estimated coefficients
]
]

---

### Class activity

.large[
.question[
How does an outlier influence the fitted regression model?
]
]

---

### Cook's distance

---

### Cook's distance in R

.large[
```{r, include=F}
x <- rnorm(100)
p <- exp(-1 + 2*x)/(1 + exp(-1 + 2*x))
y <- rbinom(100, 1, p)
```

```{r, eval=F}
x1 <- c(x, -2)
y1 <- c(y, 1)
m1 <- glm(y1 ~ x1, family = binomial)

plot(x1, cooks.distance(m1))
```

```{r, echo=F, fig.align='center', fig.width=7, fig.height=5}
x1 <- c(x, -2)
y1 <- c(y, 1)
m1 <- glm(y1 ~ x1, family = binomial)

plot(x1, cooks.distance(m1), cex = 1.2, pch=16)
```
]

---

### Cook's distance in R

.large[

```{r, eval=F}
x1 <- c(x, -5)
y1 <- c(y, 1)
m1 <- glm(y1 ~ x1, family = binomial)

plot(x1, cooks.distance(m1))
```

```{r, echo=F, fig.align='center', fig.width=7, fig.height=5}
x1 <- c(x, -5)
y1 <- c(y, 1)
m1 <- glm(y1 ~ x1, family = binomial)

plot(x1, cooks.distance(m1), cex = 1.2, pch=16)
```
]

---

### Addressing model issues

.large[
.question[
How should we handle outliers and influential points? Discuss with a neighbor for a few minutes, then we will discuss as a group.
]
]
